# llama.cpp mit AMD GPU (Vulkan Backend)

Dieses Repository enth√§lt die Konfiguration und Dokumentation f√ºr llama.cpp mit Vulkan-Unterst√ºtzung f√ºr AMD GPUs unter Linux.


## üéØ Vulkan vs ROCm

**Vulkan:** Universal, einfach, schnell f√ºr Inferenz ‚úÖ  
**ROCm:** Nur AMD, komplex, besser f√ºr Training ‚ö†Ô∏è

**‚Üí F√ºr llama.cpp: Nutze Vulkan**

---

## üì¶ Voraussetzungen

### System
- Linux (Ubuntu/Debian/Arch/etc.)
- AMD GPU mit Vulkan-Unterst√ºtzung
- Aktuelle GPU-Treiber (AMDGPU/Mesa)

### Packages installieren

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install -y \
    cmake \
    build-essential \
    git \
    libvulkan-dev \
    vulkan-tools \
    mesa-vulkan-drivers
```

**Arch Linux:**
```bash
sudo pacman -S cmake base-devel git vulkan-headers vulkan-icd-loader mesa
```


### Vulkan-Unterst√ºtzung pr√ºfen
```bash
vulkaninfo | less 
```
Du solltest deine AMD GPU sehen.

---

## üî® Installation & Build

### 1. Repository klonen (falls noch nicht vorhanden)
```bash
git clone https://github.com/ggerganov/llama.cpp.git .
```

### 2. Clean Build mit Vulkan
```bash
rm -rf build
cmake -B build -DGGML_VULKAN=ON
cmake --build build -j$(nproc)
```

**Build-Optionen:**
- `-DGGML_VULKAN=ON` - Aktiviert Vulkan-Backend f√ºr GPU-Beschleunigung
- `-j$(nproc)` - Nutzt alle CPU-Kerne f√ºr schnelleres Kompilieren

### 3. Build √ºberpr√ºfen
```bash
./build/bin/llama-cli --version
```

---

## üì• Modelle herunterladen

### Empfohlene Modelle

#### Meta Llama 3.1 8B Instruct (Q4_K_M)
Gutes Allround-Modell, optimiert f√ºr Instruktionen.

```bash
mkdir -p models/llama
cd models/llama
wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
```

**Direkter Link:**
https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/blob/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf

#### GPT-OSS 20B
Gr√∂√üeres, leistungsf√§higeres Modell.

```bash
mkdir -p models/gpt-oss
cd models/gpt-oss
# Modell von hier herunterladen:
# https://huggingface.co/ggml-org/gpt-oss-20b-GGUF/tree/main
```

### Quantisierungsstufen erkl√§rt

- **Q4_K_M**: Gutes Gleichgewicht zwischen Qualit√§t und Geschwindigkeit (empfohlen)
- **Q5_K_M**: Bessere Qualit√§t, etwas langsamer
- **Q8_0**: H√∂chste Qualit√§t, deutlich gr√∂√üer und langsamer
- **Q2_K**: Sehr klein und schnell, niedrigere Qualit√§t

---

## üöÄ Server starten

### Llama 3.1 8B Server
```bash
./build/bin/llama-server \
    -m /mnt/expansion/02_KI/07_llamacpp/models/llama/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
```

**Server-Ausgabe:**
```
main: server is listening on http://127.0.0.1:8080 - starting the main loop
```

### Erweiterte Optionen
```bash
./build/bin/llama-server \
    -m models/llama/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf \
    -c 4096 \           # Context-Gr√∂√üe
    -ngl 99 \           # GPU-Layers (99 = alle)
    --host 0.0.0.0 \    # Auf allen Interfaces lauschen
    --port 8080         # Port √§ndern
```

**Wichtige Parameter:**
- `-m` - Pfad zum Modell
- `-c` - Context-L√§nge (Standard: 2048)
- `-ngl` - Anzahl der Layers auf GPU (99 = alle verf√ºgbaren)
- `--host` - Server-Adresse (0.0.0.0 f√ºr Netzwerkzugriff)
- `--port` - Server-Port


